{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbcb86d",
   "metadata": {},
   "source": [
    "# Run Hugging Face `facebook/opt-13b` autoregressive sampling on Inf2 & Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf3d85",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [facebook/opt-13b](https://huggingface.co/facebook/opt-13b) model for tensor parallel inference on Neuron using the `transformers-neuronx` package.\n",
    "\n",
    "The example has the following main sections:\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Download and construct the model\n",
    "1. Split the model `state_dict` into multiple files\n",
    "1. Perform autoregressive sampling using tensor parallelism\n",
    "\n",
    "This Jupyter Notebook should be run on an Inf2 instance (`inf2.8xlarge` or larger) or a Trn1 instance (`trn1.32xlarge`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b32cf9",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5da6a7",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`facebook-opt-13b-sampling.ipynb`) and launch it. Follow the rest of the instructions in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805bd56",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de11e14",
   "metadata": {},
   "source": [
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/setup/setup-inference.html). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/aws-neuron/transformers-neuronx.git transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032b339",
   "metadata": {},
   "source": [
    "## Download and construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a93feb",
   "metadata": {},
   "source": [
    "We download and construct the `facebook/opt-13b` model using the Hugging Face `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab87fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2373e08e81d24a87b5b8c3f784ce0f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.models.opt import OPTForCausalLM\n",
    "\n",
    "# hf_model = OPTForCausalLM.from_pretrained('facebook/opt-13b', low_cpu_mem_usage=True)\n",
    "hf_model = OPTForCausalLM.from_pretrained('facebook/opt-6.7b', low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a37ba2",
   "metadata": {},
   "source": [
    "## Split the model state_dict into multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e351",
   "metadata": {},
   "source": [
    "For the sake of reducing host memory usage, it is recommended to save the model `state_dict` as\n",
    "multiple files, as opposed to one monolithic file given by `torch.save`. This \"split-format\"\n",
    "`state_dict` can be created using the `save_pretrained_split` function. With this checkpoint format,\n",
    "the Neuron model loader can load parameters to the Neuron device high-bandwidth memory (HBM) directly\n",
    "by keeping at most one layer of model parameters in the CPU main memory.\n",
    "\n",
    "To reduce memory usage during compilation and deployment, we cast the attention and mlp to `float16` precision before saving them. We keep the layernorms in `float32`. To do this, we implement a callback function that casts each layer in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13d8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "\n",
    "def amp_callback(model, dtype):\n",
    "    # cast attention and mlp to low precision only; layernorms stay as f32\n",
    "    for block in model.model.decoder.layers:\n",
    "        block.self_attn.to(dtype)\n",
    "        block.fc1.to(dtype)\n",
    "        block.fc2.to(dtype)\n",
    "    model.lm_head.to(dtype)\n",
    "\n",
    "amp_callback(hf_model, torch.float16)\n",
    "save_pretrained_split(hf_model, './opt-6.7b-split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac11413",
   "metadata": {},
   "source": [
    "## Perform autoregressive sampling using tensor parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097b88a",
   "metadata": {},
   "source": [
    "Now we have all of the necessary files for running `facebook/opt-13b` autoregressive sampling. \n",
    "\n",
    "To get a large language model working on Inf2 & Trn1, tensor parallelism is used to split weights and data across multiple NeuronCores. Each NeuronCore has 16GB of memory. As a rule of thumb, the total space required per NeuronCore will be at least `2 * number of model parameters` for a `float16` casted model. In reality, the total space required is often greater due to the key value cache, which grows with sequence lenght. This memory usage determines the minimum viable instance size since the amount of memory that will be allocated on one NeuronCore is directly proportional to the parallelism degree (`tp_degree`), or rather the number of physical NeuronCores per instance. The parallelism degree must be chosen to ensure that the memory usage per NeuronCore will be less than the physical 16GB limit. While this determines the minimum instance sizing, further decreasing the memory usage per NeuronCore by using a larger instance and a higher `tp_degree` should result in a faster model\n",
    "\n",
    "We will use the Neuron `OPTForSampling` class to implement tensor parallelism. The default model config supports sampling up to sequence length 2048, and we set batch size to 2. Tensor-parallelism is enabled through the argument\n",
    "`tp_degree=2`. Internally, the Neuron tensor manipulator will shard and duplicate tensors to multiple\n",
    "NeuronCores (2 in this case) to enable tensor-parallel computations on multiple NeuronCores. The model computational graph is compiled by neuronx-cc for optimized inference on Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1cc536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/opt/model.py:36: UserWarning: torch_dtype=torch.float16 ignored in favor of amp=f16\n",
      "  warnings.warn(f'torch_dtype={config.torch_dtype} ignored in favor of amp={amp}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      ".....\n",
      "Compiler status PASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-May-20 11:58:07.0279 164578:165382 ERROR   ENC:enc_init_global_comm                    [nec_dev 0] global nec_comm is already init'd, g_device_id = 0, g_device_cnt = 2\n",
      "2023-May-20 11:58:07.0279 164578:165382 ERROR   NRT:nrt_load_collectives                    failed to create global communicator, global_device_id=0, global_device_count=1, ROOT_COMM_ID=localhost:43223)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 # load facebook/opt-13b to NeuronCores with 2-way tensor parallel</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 # enable float16 casting</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>neuron_model = OPTForSampling.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">'./opt-6.7b-split'</span>, batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, tp_degre    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 9 neuron_model.to_neuron()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/opt/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">92</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to_neuron</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 89 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>lm_head.materialize()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder_lm_head.add_lm_head(lm_head.weight.detach().T)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>lm_head.nullify()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 92 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder_lm_head.to_neuron()                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.context_length_estimate <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder_lm_head_for_context = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder_lm_head.build_weight_shared(   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>n_positions_list=[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.context_length_estimate],                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">decoder.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">89</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to_neuron</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 86 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>ln_lm_head_params.append(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head_bias)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 87 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 88 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.program = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._build_program()                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 89 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.program.setup(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layers, ln_lm_head_params)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 90 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">build_weight_shared</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, n_positions_list=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, n_active_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, batch_siz   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 92 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span>unroll=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, share_caches=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">decoder.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">683</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">setup</span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">680 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.memories = [kernel.build_memory() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> kernel <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.kernels]                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">681 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">682 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">setup</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, layers, ln_lm_head_params):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>683 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().setup(layers, ln_lm_head_params)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">684 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> npos, memory <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.n_positions_list, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.memories):                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">685 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>input_tensors = [*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.input_buffers]                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">686 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output_tensors = [<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.logits_buffer]                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">decoder.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">639</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">setup</span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">636 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.logits_buffer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.manipulator.duplicate(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.logits_buffer)                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">637 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> kernel <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.kernels:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">638 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>kernel.build()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>639 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>kernel.load()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">640 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">641 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_neff_bytes</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">642 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> [kernel.neff_bytes <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> kernel <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.kernels]                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">compiler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">264</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">261 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">262 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">263 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model = torch.classes.neuron.ParallelModel(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.neff_bytes, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.tp_degree)   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>264 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model.load()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">265 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">266 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, memory):                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">267 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ops.parallel_run(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model, memory.inputs, memory.outputs)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>nrt_load_collectives <span style=\"color: #808000; text-decoration-color: #808000\">status</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m9\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m# load facebook/opt-13b to NeuronCores with 2-way tensor parallel\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m# enable float16 casting\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0mneuron_model = OPTForSampling.from_pretrained(\u001b[33m'\u001b[0m\u001b[33m./opt-6.7b-split\u001b[0m\u001b[33m'\u001b[0m, batch_size=\u001b[94m1\u001b[0m, tp_degre    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 9 neuron_model.to_neuron()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/opt/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m92\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mto_neuron\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 89 \u001b[0m\u001b[2m│   │   \u001b[0mlm_head.materialize()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.decoder_lm_head.add_lm_head(lm_head.weight.detach().T)                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   │   \u001b[0mlm_head.nullify()                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 92 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.decoder_lm_head.to_neuron()                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.context_length_estimate \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.decoder_lm_head_for_context = \u001b[96mself\u001b[0m.decoder_lm_head.build_weight_shared(   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mn_positions_list=[\u001b[96mself\u001b[0m.context_length_estimate],                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/\u001b[0m\u001b[1;33mdecoder.py\u001b[0m:\u001b[94m89\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mto_neuron\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 86 \u001b[0m\u001b[2m│   │   │   \u001b[0mln_lm_head_params.append(\u001b[96mself\u001b[0m.lm_head_bias)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 87 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 88 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.program = \u001b[96mself\u001b[0m._build_program()                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 89 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.program.setup(\u001b[96mself\u001b[0m.layers, ln_lm_head_params)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 90 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 91 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mbuild_weight_shared\u001b[0m(\u001b[96mself\u001b[0m, n_positions_list=\u001b[94mNone\u001b[0m, n_active_tokens=\u001b[94mNone\u001b[0m, batch_siz   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0munroll=\u001b[94mNone\u001b[0m, share_caches=\u001b[94mFalse\u001b[0m):                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/\u001b[0m\u001b[1;33mdecoder.py\u001b[0m:\u001b[94m683\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92msetup\u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m680 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.memories = [kernel.build_memory() \u001b[94mfor\u001b[0m kernel \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.kernels]                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m681 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m682 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92msetup\u001b[0m(\u001b[96mself\u001b[0m, layers, ln_lm_head_params):                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m683 \u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().setup(layers, ln_lm_head_params)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m684 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m npos, memory \u001b[95min\u001b[0m \u001b[96mzip\u001b[0m(\u001b[96mself\u001b[0m.n_positions_list, \u001b[96mself\u001b[0m.memories):                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m685 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_tensors = [*\u001b[96mself\u001b[0m.input_buffers]                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m686 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput_tensors = [\u001b[96mself\u001b[0m.logits_buffer]                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/\u001b[0m\u001b[1;33mdecoder.py\u001b[0m:\u001b[94m639\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92msetup\u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m636 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.logits_buffer = \u001b[96mself\u001b[0m.manipulator.duplicate(\u001b[96mself\u001b[0m.logits_buffer)                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m637 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m kernel \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.kernels:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m638 \u001b[0m\u001b[2m│   │   │   \u001b[0mkernel.build()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m639 \u001b[2m│   │   │   \u001b[0mkernel.load()                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m640 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m641 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mget_neff_bytes\u001b[0m(\u001b[96mself\u001b[0m):                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m642 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m [kernel.neff_bytes \u001b[94mfor\u001b[0m kernel \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.kernels]                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/\u001b[0m\u001b[1;33mcompiler.py\u001b[0m:\u001b[94m264\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mload\u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mload\u001b[0m(\u001b[96mself\u001b[0m):                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m263 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.model = torch.classes.neuron.ParallelModel(\u001b[96mself\u001b[0m.neff_bytes, \u001b[96mself\u001b[0m.tp_degree)   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m264 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.model.load()                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m265 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m266 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__call__\u001b[0m(\u001b[96mself\u001b[0m, memory):                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m267 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m ops.parallel_run(\u001b[96mself\u001b[0m.model, memory.inputs, memory.outputs)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mnrt_load_collectives \u001b[33mstatus\u001b[0m=\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.opt.model import OPTForSampling\n",
    "\n",
    "# load facebook/opt-13b to NeuronCores with 2-way tensor parallel\n",
    "# enable float16 casting\n",
    "neuron_model = OPTForSampling.from_pretrained('./opt-6.7b-split', batch_size=2, tp_degree=2, amp='f16')\n",
    "neuron_model.to_neuron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d1682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated sequences [\"</s>Hello, I'm a language model, and this post is gay.\\nAre you by any chance the language model from the Matrix?\\nYou are the first to point it out!\\nHow come you never made your own username? Did you want to seem mysterious?  Now there's no need.\\nMystery is always an attractive quality. I did have one before..\\nI'm going to pretend we were close acquaintances, then.\\nClose enough for your AMA.\\nI am a newbie. Do AMA? I have no idea how the site functions.   And my work doesn't allow me to use computers.\\nAMA means Ask Me Anything. I asked you questions, and you answered.\\nThank you. I appreciate the help.\\nHow can people respond faster than I do without computers?\\nThis is a good question. I have no idea.\\nI'd also like to know your answer.\\nA computer is a computer, isn't it?\\nIf you don't know your response, maybe it's time to go back to asking the question. Or, y'know, just use a computer, you idiot.\\nIt is late, and I need to sleep. You people are rude.\\nI was just being a jerk and saying your lack of computer skills might be an opportunity to find a new career.  You should use a computer for research. It can show you things you didn't know existed.\\nI know how to use them, I just think they are kind of stupid.   (not really)  The things you discover are neat, but I find them boring...  I am better at other arts.   But, I also know about computers, as they are part of my job.\\nI'm glad you are better at other things. Some people never find their niche in life.   I enjoy playing instruments and I draw and paint. Not super great, but I'll never be an artist. I wish I was because I know the feeling of not being good at it.\\nI'm pretty talented, but I don't show off.\\nWell, you should. It's okay to be passionate about things. Just own it.\\nI am just too shy.\\nJust be honest and confident. Some people are hard to get to a point where you feel comfortable with them showing that they're confident enough.\\nI just don't want people to think I am boastful.\\nYou're the one doing the AMA. Not everyone will be as open as you. But when you show it, it's just a smile or a gesture. It doesn't say anything about you as a person.\\nThere's being honest and then there's being boastful. I'm probably the latter.\\nThere's always two sides of every story.   I don't think you are.\\nThanks.  You should see if people want to AMA about things too.\\nThere have been a few AMAs done in here.  They aren't as popular as people thought they would be.\\nI see. There really isn't much to AMA about? I suppose people don't really like to be asked anything much.\\nI think people prefer to answer their questions or post in subreddits like askreddit or something.  But hey, if it's a good idea I'd participate.\\nAnd I have no time...   I'm just a kid, and I'm sure people are going to do that when I can. However, I would prefer a group AMA if it were to happen. I have friends who could have something interesting to say.\\nYou should. Maybe someone else would see it as a good idea as well.\\nPerhaps. When I have time, I'll certainly come back and do a group AMA. Thanks again for your help.\\nNo problem. I'll be here.   If you want to do it without doing another AMA, you can also send an email to  ask us anything at ask.ask.com and have it answered. I am not sure if it would be upvoted to get the attention of the whole subreddit.\\nIf you have time, consider doing it at /r/casualama. They do lots of things that don't necessarily involve having to do a full AMA. Ask.com is just the easiest option for the person that was being asked the question.\\nI agree. I may do it there.  For the time being, I think I will leave it be, and let people start asking questions in here. I do like this subreddit, though.  I shall check out ask.com when the time comes.\\nSure thing. Good point about this subreddit.   But, as I said, don't be afraid to send an email. It would be appreciated.\\nWill do. I've sent some to /r/casualama.  I was afraid to get downvoted by everyone who comes here.\\nIt's fine. Everyone understands you are a newbie that needs a little help.\\nThat is awesome. It was a bit nerve wrecking, but it feels good to know I can reach out to people like this if I need it.\\nWell, it's quite obvious you need the help. You are very inexperienced to this point and it's awesome that you were able to find this out from experience. It's also nice to know people who are a little more experienced who are able to help you.   I hope you get the attention you want and get some cool, fun answers.\\nI do, too.   I'm going to go to the beach for awhile. I think I need to get out of school and relax for a bit.   Thank you.\\nBeach? Who are you, John Travolta?\\nLOL! No. I'm just doing what a lot of people say you should and get out and relax sometimes. Especially being in school. There's enough stress with all that.\\nOh, I was just kidding around. I get what you mean. Good idea though. Just make sure to clean your sunscreen off before swimming. It can turn you white. :P\\nOh I know. It's not an every day thing.  It is what I wanted to do today though. I have finals coming this week, and I feel that I am ready for anything... Except the dreaded AP History, which has been going on for months...  Thanks. I feel a lot better now. I know now, how I should do it differently.\\nJust make sure to have someone bring you food and drink. You will be dead from dehydration. LOL\\nLOL! I have a cool cousin that works there.  She is going to bring some nice iced tea. I should stay hydrated, though. That is an important thing to remember.\\nBe sure to come back and chat some more.\\nWill do:)   Thanks man.  Have a nice day, and good luck with your finals.\\nI will. Just take care with your drinking and swimming.   Have a nice day :D!\\nI will. Have a good day. I will see you around sometime.   Have a nice day! Be safe.\\nSure thing. Good luck with the future! Happy Thanksgiving :)!\\nI have to say that, for a person that isn't even American, you do have a nice vocabulary.  I also thank a fellow student for a job well done before I head to class!   (That really is nice of you, by the way!)  Have a good night! :D! It isn't quite Thanksgiving yet. :D!\\nThank you! I really appreciate that.   You do some good around here. :D! I hope your finals are not too horrible! But also hope you have a good life outside of school as well.  Have a good Thanksgiving, too! :D! Have a safe holiday season! :D!\\nLikewise!   And I hope you have some nice finals as well. You do some good too. :)! You have a really nice smile. Happy days to you. :D!\\nThank you. I appreciate it. Happy days to you, as well! :D! I hope the best for you, as well. :)! The future will be alright!  Have a great holiday season! :D! Enjoy your nice smile! You seem like a bright person. :)!   EDIT: That went on for a little too long. :D!\\nThat is what I like to hear. :)! You did a great job tonight too. Thanks for that!  Thanks for everything!  I will do my best with finals. I hope your holidays are enjoyable without lots of stress. :)! I only know this because I've got some right now, too! :D!  Have a nice end of December! We might see each other again. :D!\\nI'd rather make new friends on Reddit or in a different sub-reddit than keep running into the same people. :)! Have a wonderful holiday season. :D!\\nOh I'm sure we will! :D! Thanks for everything! I hope you have a fantastic holiday season! :D! I'll see you around! :D!  EDIT: No, I meant you're great. I just like to talk to people. :D!\\nSee you around! :D! Thanks for the compliment. Have a fantastic holiday season! :D! I hope my finals go well. :D! Hopefully I'll see you later! :D!\\nI'll be around for sure! I like talking to people on Reddit! I'll be around for some time. :)! Have a fantastic day! :D! And I hope you do well on your finals! :)! I'll be around to wish you good luck. And I'll be around for sure to welcome you to the new semester. :D!\\nI would like to\", '</s>Welcome to Amazon Elastic Compute Cloud, a fully managed computing service. Amazon EC2 offers the power of the world’s largest datacenter with no upfront capital investment and easily scaled-up or scaled-out capacity. Use Amazon EC2 to increase the effectiveness of a developer’s code when you are developing applications in many different languages like Ruby, PHP, Java, Python, Go, Ruby on Rails,.Net, PHP, Perl, Groovy, Erlang, Tcl, Go, Scala, Objective-C, and many others.\\n\\nAmazon EC2 gives you access to the same Linux instances used in Amazon web services. EC2 instances are designed to handle high throughput and are specially configured for running your database. We will build an instance for you but first, you need to decide what type of database you want. Amazon RDS offers a scalable, secure, and scalable relational database service, including the ability to quickly launch and run a database on-demand for just $5/month. If you already have a MySQL server running on a Linux PC or server, AWS RDS makes it easy to transfer your existing data to Amazon RDS without losing any of your data.\\n\\nIf you are a developer that is building a Ruby on Rails web application with Ruby 1.9.2, Amazon Elastic Load Balancing makes it easy to move requests from one of your front end web servers to another without worrying about downtime. When your web application is built, upload your Gemfile.lock file to Amazon ELB. To ensure the availability of the application and ensure Ruby is correctly run on the application. You can also run rails server with the --server-port=2047 to run your application.\\n\\nOnce the application is ready, upload your application from your computer. To simplify your application, use a publically available source available on Amazon EC2 to upload the source. Next, launch an Amazon Elastic Container Registry (ECR) from the command line in the same environment you are developing in. In the Amazon EC2 console, right-click the image that contains your application. In the command line, type the command below and then click Launch.\\n\\n$ sudo bash -c \\'bash -c \"ssh -I.ssh/config user@ec2-10-30-49-61.compute.amazonaws.com./myapp\" $ sudo mv image.md5 image.png\\'\\n\\nWe don\\'t provide your source code files but instead provide you with an image. An image is the base image that you can use to build your application. An image is always updated with the latest code. You can select a file from the command line to upload to Amazon ECR. On the image, you can specify options of the container such as the name of the container.\\n\\n$./myapp -c -i \\'./index.html\\'\\n\\nOnce your application is ready, you can access it using the endpoint /myapp. For example, the URL for the application on the Internet will be http://ec2-10-30-49-61.compute-amz.us-west-2.compute.amazonaws.com/myapp. As long as you have uploaded a valid image, you will be able to easily access and browse through your app.\\n\\nOnce you finish using the Amazon EC2 console, you can delete your EC2 instance. By default, you won’t have access to the EC2 console while your EC2 instance is removed.\\n\\nOnce you are familiar with Amazon EC2, you can access it using any computer. For more information about EC2, visit http://aws.amazon.com/ec2.\\n\\nThe information provided at the following page is provided by Amazon Web Services, Inc. (AWS), which is solely responsible for its accuracy. The specifications provided do not limit the application of the invention. AWS makes no representations, warranties, or guarantees about the accuracy, completeness, or adequacy of the content, nor does AWS guarantee the condition or security of the server to which the invention applies. AWS cannot and does not warrant that the invention will work for you or achieve the intended results; you assume full responsibility for your use of AWS.\\n\\nThe foregoing examples of the invention have been provided to illustrate the invention and are not intended to limit the invention. The scope of the invention is defined by the claims that follow.\\n\\nShare:\\n\\nRelated\\n\\nIf you are new here, you may want to subscribe to my RSS feed or subscribe to get new articles delivered by email on this subject. You may want to follow me on Twitter. Or leave me a comment! I always love to hear from you! If you have a blog and would like me to check it out, I would love to! Please go to My Blog Setup in the right panel.\\n\\nComments\\n\\nHi Richard,\\nIn the AWS console, after logging in, there is a button that says “launch image”. If you click on that, it will take you to a page with a list of images that would be suitable for deploying a specific server. After you have selected an image, it will let you launch it.\\nI hope it helps.\\n\\nNice, so you can deploy a new instance in the Amazon instance from the EC2 console.\\nThe question is how to remove an Amazon instance once you are done with it? By removing “Amazon EC2 Console” is there a way to just remove the instance? Or must log in again in the EC2 console and remove it?\\n\\nHi Daniel\\nTo remove an instance, just log in the ec2 console and right-click the instance. If it’s in your current location, then it will give you an option to delete it. Otherwise, you may need to do an action. The action menu for EC2 will give you different options including “Delete”. Once you click the delete option, the ec2 console will display your instance. You will click on the instance and it will display the following page with the option to delete the instance.\\nBest Regards\\nRichard\\n\\nI am just learning about AWS, and was excited to find this blog, and especially the very helpful information presented. I have a question as I am not familiar at all with the ec2 console, can I download and install a server from ec2 directly? What kind of linux server are you familiar with, as I can not find any that are just for web service. I see a few that are server images. Can I just take this image, install it on a computer and deploy a server with this.\\nThanks,\\nTravis\\n\\nYes, you can choose to launch an image. There is one called AMAZON-EC2-BASED-UNTARED-SERVER-IEC63659-1-standard that would be suitable. In the console you will see two choices:\\nDeploy\\nDeploying an entire, untarred server image will automatically create and start an Amazon web server on this server.\\nThe Amazon Machine Image (AMI) for the server instance is automatically downloaded and downloaded and attached to the Amazon EC2 virtual machine.\\n\\nHi Richard,\\nAfter you launch it again in a few minutes, it will tell you the instance name. You can then go to Cloud Computing in the sidebar and select a new EC2 in the drop-down menu. There are a lot of EC2 servers to choose from. You should be able to find one that fits your needs.\\nBest Regards\\nRichard\\n\\nThis is very very good info!\\nI have already got the instances for my applications, but did not know about this (the image).\\nBut..\\nI run a linux box. I always want my image uploaded and this way, if i deploy a new server, i will just get the image.\\n\\nThe aws console was so easy to navigate, even if I have no experience, was really well explained and it is in really clear way. I got it in no time.\\nIf any one has questions please let me know.\\n\\ni have 3 EC2 instances with this software, I wish to see how many times I’m running each instance. Is there a way to graph it, because all the time i don’t know how many my instances are using.\\n\\nHello, I was really intrigued and I really liked this post. But I have a question. When I use your code, it says that the RDS instance is created after installing the AMI but I don’t use the AMI for running the code. I run my own images and I already have databases. Is there something I am missing? Thank you.\\n\\nHi Jose. Yes you can attach an RDS instance after deploying. You can choose the database option from the deployment step (under Deploy). Once the deployment is complete, you will have an RDS instance added to the Amazon EC2 console.\\nThank you\\n\\nRichard,\\nDo you recommend that you use the images in the “deploy” menu because that is the first place I tried looking for and didn’t find it\\n\\nOr is it because I am not really familiar with how RDS runs. I could have misunderstood the explanation. So is there a way the image can be attached after deployment and not need to go from the “deploy” step\\n\\nHi Brian.\\nI have made a change to the post. The images are now listed directly. Thanks for pointing that out.\\nThe images are only listed under Deploy because that is where I listed them initially.\\nBest Regards\\nRichard\\n\\nI want to try this out and try to develop with Ruby on Rails. I have been learning and practicing for a while now, but haven’t been able to successfully deploy the code onto my server.\\n'] in 82.91460514068604 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# construct a tokenizer and encode prompt text\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-6.7b')\n",
    "batch_prompts = [\n",
    "    \"Hello, I'm a language model,\",\n",
    "    \"Welcome to Amazon Elastic Compute Cloud,\",\n",
    "]\n",
    "input_ids = torch.as_tensor([tokenizer.encode(text) for text in batch_prompts])\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=2048)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n",
    "print(f'generated sequences {generated_sequences} in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60f67a",
   "metadata": {},
   "source": [
    "Larger batch sizes won't fit into an `inf2.8xlarge` or instance. These instances have 32 GB of HBM, and `facebook/opt-13b` has ~26 GB of model parameters in `float16`. With batch size 3, after storing model parameters and key-value caches, there will be less than 1 GB of HBM left, which is not enough for storing code and temporary data generated during the sampling computation. \n",
    "\n",
    "To use larger batch sizes, please consider using an `inf2.48xlarge` or  `trn1.32xlarge`. You can also try using a larger tensor parallelism degree, such as 8, on an `inf2.48xlarge` or a `trn1.32xlarge`. The `facebook/opt-13b` number of attention heads is 40, so the tensor parallelism degree must be a divisor of 40 and be supported on the Inf2 or Trn1 instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
