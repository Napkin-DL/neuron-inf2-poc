{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbcb86d",
   "metadata": {},
   "source": [
    "# Run Hugging Face `facebook/opt-13b` autoregressive sampling on Inf2 & Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf3d85",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [facebook/opt-13b](https://huggingface.co/facebook/opt-13b) model for tensor parallel inference on Neuron using the `transformers-neuronx` package.\n",
    "\n",
    "The example has the following main sections:\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Download and construct the model\n",
    "1. Split the model `state_dict` into multiple files\n",
    "1. Perform autoregressive sampling using tensor parallelism\n",
    "\n",
    "This Jupyter Notebook should be run on an Inf2 instance (`inf2.8xlarge` or larger) or a Trn1 instance (`trn1.32xlarge`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b32cf9",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5da6a7",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`facebook-opt-13b-sampling.ipynb`) and launch it. Follow the rest of the instructions in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805bd56",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de11e14",
   "metadata": {},
   "source": [
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/setup/setup-inference.html). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69d635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/aws-neuron/transformers-neuronx.git transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032b339",
   "metadata": {},
   "source": [
    "## Download and construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a93feb",
   "metadata": {},
   "source": [
    "We download and construct the `facebook/opt-13b` model using the Hugging Face `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab87fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b857954abc4786b4ec7bc7ab1175da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers.models.opt import OPTForCausalLM\n",
    "\n",
    "# hf_model = OPTForCausalLM.from_pretrained('facebook/opt-13b', low_cpu_mem_usage=True)\n",
    "hf_model = OPTForCausalLM.from_pretrained('facebook/opt-6.7b', low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a37ba2",
   "metadata": {},
   "source": [
    "## Split the model state_dict into multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e351",
   "metadata": {},
   "source": [
    "For the sake of reducing host memory usage, it is recommended to save the model `state_dict` as\n",
    "multiple files, as opposed to one monolithic file given by `torch.save`. This \"split-format\"\n",
    "`state_dict` can be created using the `save_pretrained_split` function. With this checkpoint format,\n",
    "the Neuron model loader can load parameters to the Neuron device high-bandwidth memory (HBM) directly\n",
    "by keeping at most one layer of model parameters in the CPU main memory.\n",
    "\n",
    "To reduce memory usage during compilation and deployment, we cast the attention and mlp to `float16` precision before saving them. We keep the layernorms in `float32`. To do this, we implement a callback function that casts each layer in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13d8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "\n",
    "def amp_callback(model, dtype):\n",
    "    # cast attention and mlp to low precision only; layernorms stay as f32\n",
    "    for block in model.model.decoder.layers:\n",
    "        block.self_attn.to(dtype)\n",
    "        block.fc1.to(dtype)\n",
    "        block.fc2.to(dtype)\n",
    "    model.lm_head.to(dtype)\n",
    "\n",
    "amp_callback(hf_model, torch.float16)\n",
    "save_pretrained_split(hf_model, './opt-6.7b-split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac11413",
   "metadata": {},
   "source": [
    "## Perform autoregressive sampling using tensor parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d097b88a",
   "metadata": {},
   "source": [
    "Now we have all of the necessary files for running `facebook/opt-13b` autoregressive sampling. \n",
    "\n",
    "To get a large language model working on Inf2 & Trn1, tensor parallelism is used to split weights and data across multiple NeuronCores. Each NeuronCore has 16GB of memory. As a rule of thumb, the total space required per NeuronCore will be at least `2 * number of model parameters` for a `float16` casted model. In reality, the total space required is often greater due to the key value cache, which grows with sequence lenght. This memory usage determines the minimum viable instance size since the amount of memory that will be allocated on one NeuronCore is directly proportional to the parallelism degree (`tp_degree`), or rather the number of physical NeuronCores per instance. The parallelism degree must be chosen to ensure that the memory usage per NeuronCore will be less than the physical 16GB limit. While this determines the minimum instance sizing, further decreasing the memory usage per NeuronCore by using a larger instance and a higher `tp_degree` should result in a faster model\n",
    "\n",
    "We will use the Neuron `OPTForSampling` class to implement tensor parallelism. The default model config supports sampling up to sequence length 2048, and we set batch size to 2. Tensor-parallelism is enabled through the argument\n",
    "`tp_degree=2`. Internally, the Neuron tensor manipulator will shard and duplicate tensors to multiple\n",
    "NeuronCores (2 in this case) to enable tensor-parallel computations on multiple NeuronCores. The model computational graph is compiled by neuronx-cc for optimized inference on Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1cc536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/opt/model.py:36: UserWarning: torch_dtype=torch.float16 ignored in favor of amp=f16\n",
      "  warnings.warn(f'torch_dtype={config.torch_dtype} ignored in favor of amp={amp}')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......\n",
      "Compiler status PASS\n",
      "2023-May-21 03:20:31.0700 254538:256038 [0] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2023-May-21 03:20:31.0700 254538:256038 [0] init.cc:99 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "......\n",
      "Compiler status PASS\n",
      "......\n",
      "Compiler status PASS\n",
      "......\n",
      "Compiler status PASS\n",
      ".......\n",
      "Compiler status PASS\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.opt.model import OPTForSampling\n",
    "\n",
    "# load facebook/opt-13b to NeuronCores with 2-way tensor parallel\n",
    "# enable float16 casting\n",
    "neuron_model = OPTForSampling.from_pretrained('./opt-6.7b-split', batch_size=2, tp_degree=2, amp='f16')\n",
    "neuron_model.to_neuron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d1682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated sequences [\"</s>Hello, I'm a language model, I was able to deduce this is about a cat\\nHear me out. It could be something even worse. It could be a cat!   I don't like cats and this makes me angry!   Cats are bad!\\nThis is the most cat thing I've ever heard and I love it.\\n>I'd like to get away from the world  >I don't like cat things  >I'm a language model  >It might be about a cat  >Cat is bad  My logic checks out guys, Cat is bad.\\n/r/catfree\\n*unsubscribe*\\nPlease don't do that.  It's not like /r/antifeatures is just a huge and pointless circle jerk.  There are a lot of great cat-less subs.\\nI am subscribed to /r/unresolvedmysteries and /r/birdswitharms, should I be unsubscribing then?\\nMaybe.  /r/birdswitharms is amazing.  Don't be ashamed to cross post.\\nOk thanks for the clarification. You see this shit *all the time* and you're supposed to ask /u/TheFencingCoach when it's safe to subscribe again.\\nHaha! I feel honored.  Thanks.  I'm not sure there's anything I don't know about this one.  I love it!\\nThank mr. fencingcoach\\nMr. Coaches, *and* thank /u/TheFencingCoach. All I have done thus far is respond to the bot.\\nJust as a question, does it just post the same comment every thread?\\nYes.  It's what it does.  You should probably take it to a different sub.\\nThat's just too bad, this bot is great. Too bad it's not funny.\\nIt will grow.  It's still a young bot.  I'll bring an upboat over and suggest r/funny as it grows.\\nGood to know. Thank mr fencingcoach.\\nr/TheFencingCoach  You're welcome!\\nI'm so confused right now. That bot seems to think you're someone else.\\nIt does that whenever I name my posts!  I don't know how it works!  I'm sorry about that, I think I figured out how to stop it.  I'll post a comment with a more accurate name so you can see.  Sorry again!\\nOh ok! Good,  thanks for the info mr fencingcoach!\\nMmkay?  Sorry about that!!  I'm glad it's starting to work.\\nThat's alright mr fencingcoach!  Thank mr thefencingcoach!  Edit: wow that's a shitty name\\nThank TheFencingCoach!  No need to worry!  And I didn't even see that!  I thought it was an edit of mine!!  I don't know who's at fault here...  I think we're both!  Sorry, please continue.\\nOk so now the bot thinks that you need to ask mr fencingcoach something, and is going to stop replying till you do!  /r/The_Fencing_Coach\\n/r/thefencingcoach is now the correct place to direct questions, unless you know TheFencingCoach, in which case you can PM them your question.  It's going to go back & forth until we're all satisfied.  Thanks.\\nLol thanks mr fencingcoach!\\nI wasn't able to do it though, it's stuck on you.  /u/MrFencingCoach, please try again, and we'll see what happens.  You know what to add to your post, if you want, to make it clear that you're replying to me.  Thank you.  MrTheFencingCoach  Thank *you*!\\nSorry for the trouble mr fencingcoach!  You're welcome!   Edit: aha,  here's my new post.\\nPerfect!  I'll do what I can to help.  Have to go to the store after this.  If you want to continue this conversation, /u/MrFencingCoach, tell me how.\\nOk well I'll still do it that way for a little  mr fencingcoach!  Mmkay! Thank mr u/Mr_FencingCoach!\\nI just hope the bot doesn't stop asking you questions!\\nLol no problem mr fencingcoach! Sorry to keep you waiting!  You can just tell it that you're being asked another question right here.  For example:  /r/The_Fencing_Coach  Is not a bot,  it's me!  It'll continue to do its job as long as you don't stop asking questions.\\nAha! That did it.  I'm glad you managed to get it working!  Thanks for taking the time.  Mmkay! Thanks for teaching me.  I'll see you on /r/FencingCoach next time.\\nThank mr u/TheFencing_Coach!  You're welcome! And have a nice evening!\\nYou bet!  Mmkay! Thanks for helping me out!\\nYou're welcome!  Have a wonderful evening!\\nThank mr u/TheFencingCoach! You too! Oh wait...  *I guess I'm the one being asked to say thank you...?*  I mean, it's cool.  Let me know if there's anything I can ever do to help you out with this.  Thanks for replying!\\nHahaha!  No problem mr fencingcoach. You've been one of the biggest helpers so far. You've taught me so much. Thanks! I appreciate all the tips you've given me. You're doing a really good job with this. It's really cool that you're trying so hard to help!  Let's see what happens next. :) mrfencingcoach  /s (I hope I don't regret this)\\nAll right!  I feel like we're getting somewhere!  I think we did it!  Mr fencingcoach! Thank you for sticking with me and following my lessons! You're so great!  See that!  I'm glad I can give back to this community!  This bot is amazing!  Thank you for taking the time to teach! You've been really helpful!  Let's see what happens next!  I'll keep practicing!  Thanks again!  mrfencingcoach  /s (I hope I don't regret this)\\nWe did it!  mr fencingcoach! Thank you!  You're welcome!  Keep practicing!  It's paying off!  Oh look, there's the bot again!  /r/MrFencingCoach  /u/mr_fencingcoach mr fencingcoach!\\nYou're welcome!  You've been a great teacher! Wow, what a difference!  It's a whole new world!  /r/MrFencingCoach/mrfencingcoach  I'm glad I can be helpful.  You've been a lot of fun and have been very helpful!  Have a great day!  mr fencingcoach  /s (I hope I don't regret this)\\nAnd there goes another bot  Thank mr /u/Mr_Fencing_Coach! Oh look!  Here's another one!  /r/Mr_Fencing_Coach  /u/mr_fencingcoach /r/mr_fencing_coach That's the same guy as before.  What are the odds?  You've taught me so much!  I've learned more from you than /u/fencing_coach!  And that's true of /u/fencing_coach too!  Amazing!  Thank you so much!\\nYou should feel honored to have been taught so much!  I'm glad to be of help! And I appreciate your kind words!  I will enjoy the day while you continue to have a great night, thank you for all your help!  mr fencingcoach  /s (I hope I don't regret this)\\nMmkay! Thanks for the tip! I hope you have a wonderful night!  mr fencingcoach  /s (I hope I don't regret this)\\nThat was great, thank you so much!  You're welcome!  You were so helpful!  I hope I didn't take too long!  I thought the lesson was going to be longer?  I'll try to keep up next time!  mr fencingcoach  /s (I hope I don't regret this)\\nSo much to keep up with!  It's a lot of fun playing with the bot! I'm glad it's helping you learn!  You've been so great! Don't worry about time!  We've had so much fun!  I'll try to keep up with that!    And you should stop now because that's my job, to take over! mr fencingcoach  /s (I hope I don't regret this)\\nI'll do my best not to disappoint!  Let's see if I did it right. /r/MrFencingCoach  /u/mr_fencingcoach mr fencingcoach! You're such a great teacher, thanks for all your help!\\nGreat work!  You're doing a great job teaching!  I'm so glad we've had this opportunity to learn together!  You're a great teacher!  We have so much fun!  I'll see you\", '</s>Welcome to Amazon Elastic Compute Cloud, this guide will show you how to sign up, verify, and login to EC2.\\n\\nThe EC2 login process will walk you through the use of a browser, a computer, and a standard Amazon account to launch this instance\\n\\nThe following section will provide detailed instructions to get started.\\n\\nThe rest of this guide will use the AWS CLI to get you started.\\n\\nCreating the account\\n\\nSign-up for Amazon Linux\\n\\nThe first step in creating your account for EC2 is to sign-up for AWS Linux.\\n\\nLog into the AWS console (https://console.aws.amazon.com/)\\n\\nClick on Digital Ocean (in case you don’t see it, there are two ways to get to it.\\n\\nClick the top right hand menu\\n\\nGo and edit the menu and select DigitalOcean\\n\\nFor this guide, we’ll go with the first option.\\n\\nClick Create Now\\n\\nYou will be asked to enter your username, email, password\\n\\nIn the next screen you will be asked to confirm your email, and then you will be directed to the “My Account” page\\n\\nOnce you have logged into the account, you will be directed to the “Your Account” page\\n\\nClick the “Launch an account” link\\n\\nSelect the AWS Linux AMI to download from within the AWS EC2 console\\n\\nIf you previously launched an account, you can select Launch an account on your existing AMI.\\n\\nIf you have never launched an account, select Launch a new EC2 instance\\n\\nCreate the Security Group and the VPC\\n\\nTo launch your EC2 machine, you need a Security Group and a VPC.\\n\\nCreate a new Security Group in the EC2 console.\\n\\nIn the Security Group list, click the Create a new Security Group button\\n\\nA page will load displaying the security policy of the group\\n\\nI’ll list out what to type in each of the boxes\\n\\nType “AWSECONTROLLER” in the Accessible IP address field\\n\\nFor the Accessible port, type “80”\\n\\nType “<YOUR_VPC_IP>” in the Address field\\n\\nFor the Accessible subnet field, type “10.0.0.0/24″\\n\\nLeave the Security Group field to read “Create,” and then you will be taken to the next page\\n\\nOn this page, you will be prompted to enter your VPC IP address. In this case, your VPC IP to connect to will be the IFTTT account you created previously when you setup your server.\\n\\nOn this next page, you will be prompted to verify your account. In the box where it says “Please enter your account details”, you will be asked to enter the following:\\n\\nUSERNAME : admin\\n\\nPASSWORD : admin\\n\\nSelect “Next” and you will be taken to the next set of fields.\\n\\nYou will need to review these fields. When you are done you can click “Create” from the bottom of the page\\n\\nIt will take a few minutes for the Security Group to be created. Once created, go back to the Security Group you created previously by navigating to its name “AWSECONTROLLER”.\\n\\nYou will see that the AWS Console has a new Security group ready to be used. It should look something like this:\\n\\nSigning up with an AWS service account\\n\\nI’m going to assume you already have an AWS account.\\n\\nThis will also guide you through using an AWS service account.\\n\\nLog into the AWS console (https://console.aws.amazon.com/)\\n\\nClick the top right hand menu\\n\\nSelect DigitalOcean (in case you don’t see the next page, the guide is not setup for the correct account)\\n\\nYou will be asked to create an account. The form to do this will consist of three fields.\\n\\nEnter a valid e-mail address where you would receive a login for this account.\\n\\nEnter a valid username where you would use to log into this account.\\n\\nEnter a valid password to access this account.\\n\\nOnce these fields are populated, click “Create”\\n\\nClick “Update account” at the top right hand side of the page.\\n\\nAfter the page refreshes, you will be given access to the EC2 console\\n\\nCreating an IAM User\\n\\nThis creates a new role with read access to all EC2 instances in the region\\n\\nYou will need to grant this user access to the EC2 instances\\n\\nNavigate to the IAM section of the AWS console\\n\\nClick “+” in the top right hand corner of the screen.\\n\\nEnter a new role name named “Access-IAMKey”\\n\\nEnter “EC2” as the role type.\\n\\nEnter “EC2_Instance” as the “Instance type”\\n\\nSelect “User for this role” as the Role name.\\n\\nEnter “admin” as the access key.\\n\\nMake sure to select the “Public access.”\\n\\nOnce you click, “Create Role,” the role will be created.\\n\\nAfter it is created, click the “view instance rights” link from top right of the page that displays the role\\n\\nIf you were not a previous user, you can sign-in to AWS\\n\\nChoose an existing account\\n\\nEnter your username and password\\n\\nEntering the username, click the Sign In button.\\n\\nOn the Sign In page, you can sign-in with a “service account” or you can create a new one.\\n\\nI am going to assume you know what a service account is. Enter in a service account name. I’ll use “admin” because it’s familiar. In this example, “admin” is now the new service account called “admin”.\\n\\nWhen this service account is created, you will be directed to the Sign Up page\\n\\nClick the “Sign Up” button\\n\\nComplete the form and verify your e-mail address\\n\\nChoose a VPC IP/Network to connect to. In our example, your public IFTTT account will be the VPC IP.\\n\\nClick the button that says “Start Creating Instance”\\n\\nIt will take a few minutes to create an instance\\n\\nOn the Amazon EC2 console, navigate to the “Amazon EC2” section.\\n\\nSelect the instance type you created earlier.\\n\\nGo to “Security Groups” which is in the “Roles and Security” section\\n\\nYou should see a new group that you created named “AWSECONTROLLER” with Security Group “AWSECONTROLLER” set as its type. It should look like this:\\n\\nLogging In with the Service Account\\n\\nOnce you are logged into AWS you will have a “user@ec2.us-west-2.compute.amazonaws.com” account. This would be your AWS account with access to EC2.\\n\\nFor this example, I will use this account to log into EC2 from the IAM console.\\n\\nLogin to the AWS IAM console\\n\\nIn the first screen you will see your “user@ec2.us-west-2.compute.amazonaws.com” account. (I’ll refer to it as “Admin”\\n\\nClick the “Roles and Users” tab at the top of the page.\\n\\nClick the “edit” link below the role entry for the new role called “Access-IAMKey”\\n\\nEnter in a new group called “AWSECONTROLLER” with your new role name called “Roles”\\n\\nIf you were to do this for all 5 roles, 5 groups would be created.\\n\\nA table of the 5 groups in the security group called AWSSECONTROLLER should load up after you click the ok button.\\n\\nClick the “add” link\\n\\nAdd another user which will be the root user for the EC2 instance type you would love to launch.\\n\\nChoose “Public”\\n\\nChoose “Public“. If you click the edit option from the roles page it lists the groups that you created\\n\\nChoose the same “AWSECONTROLLER” group we just created under the roles section as the group\\n\\nClick save\\n\\nSelecting the right AWS account\\n\\nIt is important that you select the right account under the “instance location” column. In this case, you will want to select the public IFTTT account you created earlier. The IFTTT account you choose here will be the closest to the location of your virtual server.\\n\\nYou’ll be prompted for a username for your new IAM user. I’m putting in the name you created earlier for your AWS account.\\n\\nEnter a password that is no more than 8 digits.\\n\\nIn the final column, select the location for your Amazon EC2 instance.\\n\\nIn this example, I’m selecting EC2_IAM_Instance_YOUR_VPC_IP\\n\\nAfter this information is loaded onto the page, click “add”\\n\\nThe last field is the key that'] in 82.56027603149414 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# construct a tokenizer and encode prompt text\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-6.7b')\n",
    "batch_prompts = [\n",
    "    \"Hello, I'm a language model,\",\n",
    "    \"Welcome to Amazon Elastic Compute Cloud,\",\n",
    "]\n",
    "input_ids = torch.as_tensor([tokenizer.encode(text) for text in batch_prompts])\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=2048)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n",
    "print(f'generated sequences {generated_sequences} in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60f67a",
   "metadata": {},
   "source": [
    "Larger batch sizes won't fit into an `inf2.8xlarge` or instance. These instances have 32 GB of HBM, and `facebook/opt-13b` has ~26 GB of model parameters in `float16`. With batch size 3, after storing model parameters and key-value caches, there will be less than 1 GB of HBM left, which is not enough for storing code and temporary data generated during the sampling computation. \n",
    "\n",
    "To use larger batch sizes, please consider using an `inf2.48xlarge` or  `trn1.32xlarge`. You can also try using a larger tensor parallelism degree, such as 8, on an `inf2.48xlarge` or a `trn1.32xlarge`. The `facebook/opt-13b` number of attention heads is 40, so the tensor parallelism degree must be a divisor of 40 and be supported on the Inf2 or Trn1 instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
