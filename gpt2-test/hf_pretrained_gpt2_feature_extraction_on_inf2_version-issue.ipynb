{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9963bc02",
   "metadata": {},
   "source": [
    "# HuggingFace Pretrained GPT2 Feature Extraction on Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5718136",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to compile and run a HuggingFace ğŸ¤— Transformers GPT2 model for accelerated feature extraction on Neuron. This notebook will use the [`gpt2`](https://huggingface.co/gpt2) model, which is primarily used for text generation and feature extraction. \n",
    "\n",
    "This Jupyter notebook should be run on a Trn1 instance (`trn1.2xlarge` or larger)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb46a3",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    "- `torch-neuronx`\n",
    "- `neuronx-cc`\n",
    "- `transformers`\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the Trn1 setup guide. The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220cf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install \"transformers < 4.21.0\"\n",
    "# !pip install git+https://github.com/aws-neuron/transformers-neuronx.git transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dbb7b",
   "metadata": {},
   "source": [
    "## Compile the model into an AWS Neuron optimized TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9362d",
   "metadata": {},
   "source": [
    "In the following section, we load the model and tokenizer, get s sample input, run inference on CPU, compile the model for Neuron using `torch_neuronx.trace()` and save the optimized model as `TorchScript`.\n",
    "\n",
    "`torch_neuronx.trace()` expects a tensor or tuple of tensor inputs to use for tracing, so we unpack the tokenzier output. Additionally, the input shape that's used duing compilation must match the input shape that's used during inference. To handle this, we pad the inputs to the maximum size that we will see during inference. We also use define a basic wrapper that ensures the `input_ids` and `attention_mask` kwargs are passed into the GPT2 model in the correct positions without requiring a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f136269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_neuronx.gpt2.model import GPT2ForHuggingFaceSampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8853d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 06:37:48.000807: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_6212939987563883973/MODULE_0_SyncTensorsGraph.3_6212939987563883973_ip-10-0-12-88-19a174f5-101886-5fbe29143eebc/1244f319-60d9-4315-b893-dffcbe2ad49a/MODULE_0_SyncTensorsGraph.3_6212939987563883973_ip-10-0-12-88-19a174f5-101886-5fbe29143eebc.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000211: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_5991913339558389757/MODULE_1_SyncTensorsGraph.3_5991913339558389757_ip-10-0-12-88-4b94cd12-101886-5fbe2915c7752/35cffdbf-ee1b-439e-b1e0-b9a629dc18c2/MODULE_1_SyncTensorsGraph.3_5991913339558389757_ip-10-0-12-88-4b94cd12-101886-5fbe2915c7752.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000277: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_11398946548965431583/MODULE_3_SyncTensorsGraph.3_11398946548965431583_ip-10-0-12-88-b75cf194-83767-5fbbb71f861b3/43a5052d-673b-4d65-95b7-cabaa5456d3a/MODULE_3_SyncTensorsGraph.3_11398946548965431583_ip-10-0-12-88-b75cf194-83767-5fbbb71f861b3.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000328: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_3277431082663981197/MODULE_3_SyncTensorsGraph.3_3277431082663981197_ip-10-0-12-88-4b94cd12-101886-5fbe291732b98/fe4b9dc8-f5f8-47b8-83ed-db937a4033ab/MODULE_3_SyncTensorsGraph.3_3277431082663981197_ip-10-0-12-88-4b94cd12-101886-5fbe291732b98.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000392: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_1059414514991227622/MODULE_4_SyncTensorsGraph.3_1059414514991227622_ip-10-0-12-88-30a8c29c-101886-5fbe29185bbd1/bd83117c-5159-427e-ab9b-3f5c07aabb0a/MODULE_4_SyncTensorsGraph.3_1059414514991227622_ip-10-0-12-88-30a8c29c-101886-5fbe29185bbd1.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000441: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_9290870109582453889/MODULE_1_SyncTensorsGraph.3_9290870109582453889_ip-10-0-12-88-def1cec0-83767-5fbbb71ce768e/a7761541-c3d0-4c2e-8549-dfa6588c054e/MODULE_1_SyncTensorsGraph.3_9290870109582453889_ip-10-0-12-88-def1cec0-83767-5fbbb71ce768e.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000494: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_5761379441327424118/MODULE_6_SyncTensorsGraph.3_5761379441327424118_ip-10-0-12-88-4b94cd12-101886-5fbe29198711a/498d6049-9f20-49d5-80cd-03f68c0a9f09/MODULE_6_SyncTensorsGraph.3_5761379441327424118_ip-10-0-12-88-4b94cd12-101886-5fbe29198711a.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000545: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_15171428142317499579/MODULE_7_SyncTensorsGraph.3_15171428142317499579_ip-10-0-12-88-154b1f71-101886-5fbe291ab2961/1395b818-642e-4416-8f5b-5482a5f32cfc/MODULE_7_SyncTensorsGraph.3_15171428142317499579_ip-10-0-12-88-154b1f71-101886-5fbe291ab2961.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000597: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_4172853236024427751/MODULE_5_SyncTensorsGraph.3_4172853236024427751_ip-10-0-12-88-691e8be0-83767-5fbbb721e0c88/93164e6d-1042-423f-a2b1-b197264d5d6e/MODULE_5_SyncTensorsGraph.3_4172853236024427751_ip-10-0-12-88-691e8be0-83767-5fbbb721e0c88.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000667: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_6534628959653098333/MODULE_9_SyncTensorsGraph.3_6534628959653098333_ip-10-0-12-88-154b1f71-101886-5fbe291be01d5/bf79722c-6fb1-4945-85be-091175c457d8/MODULE_9_SyncTensorsGraph.3_6534628959653098333_ip-10-0-12-88-154b1f71-101886-5fbe291be01d5.neff. Exiting with a successfully compiled graph\n",
      "2023-05-21 06:37:49.000717: INFO ||NCC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/USER_neuroncc-2.6.0.19+3d819e565/MODULE_13661006737930459022/MODULE_4_SyncTensorsGraph.3_13661006737930459022_ip-10-0-12-88-185e57d-83767-5fbbb720bbd1b/6b3e52ab-0003-4bdd-8377-71319c986ddb/MODULE_4_SyncTensorsGraph.3_13661006737930459022_ip-10-0-12-88-185e57d-83767-5fbbb720bbd1b.neff. Exiting with a successfully compiled graph\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "\n",
    "# Create a wrapper to correctly order the inputs\n",
    "class GPT2Neuron(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Ensures that `input_ids` and `attention_mask` are passed into the GPT2\n",
    "    model in the correct positions without requiring a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "# Create the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token # Define the padding token value\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2', torchscript=True)\n",
    "model = GPT2Neuron(gpt2)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get an example input\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    text,\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "example = (\n",
    "    encoded_input['input_ids'],\n",
    "    encoded_input['attention_mask'],\n",
    ")\n",
    "\n",
    "# Run inference on CPU\n",
    "output_cpu = model(*example)\n",
    "\n",
    "# Compile the model using the wrapper\n",
    "model_neuron = torch_neuronx.trace(model, example)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'model.pt'\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7056884",
   "metadata": {},
   "source": [
    "## Run inference and compare results\n",
    "\n",
    "In this section we load the compiled model, run feature extraction inference on Neuron, and compare the CPU and Neuron outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fd409a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.pt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b0d172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3041,  5372,   502,   416,   597,  2420,   345,  1549,   588,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb337e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuronModule(original_name=NeuronModule)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.modeling_utils.get_parameter_dtype = lambda x: torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b56563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU outputs:    tensor([ 0.1629, -0.2166, -0.1410,  0.0061, -0.0623, -0.2181, -0.8142, -0.0920,\n",
      "        -0.3586,  0.0676], grad_fn=<SliceBackward0>)\n",
      "Neuron outputs: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "# Load the TorchScript compiled model\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "output_neuron = model_neuron(*example)\n",
    "\n",
    "# Compare the results\n",
    "print(f\"CPU outputs:    {output_cpu[0][0][0][:10]}\")\n",
    "print(f\"Neuron outputs: {output_neuron[0][0][0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4b10f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e35f54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2Model.from_pretrained('gpt2', torchscript=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6353ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "\n",
    "class GPT2Neuron(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Ensures that `input_ids` and `attention_mask` are passed into the GPT2\n",
    "    model in the correct positions without requiring a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "# Create the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token # Define the padding token value\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2', torchscript=True)\n",
    "# model = GPT2Neuron(gpt2)\n",
    "save_pretrained_split(gpt2, './gpt2-split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34fb64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 # load facebook/opt-13b to NeuronCores with 2-way tensor parallel</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 # enable float16 casting</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>neuron_model = GPT2ForHuggingFaceSampling.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">'./gpt2-split'</span>, batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>,     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>11 neuron_model.to_neuron()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">222</span>  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">to_neuron</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">219 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>layer.materialize()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">220 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>attn = layer.attn                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">221 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>mlp = layer.mlp                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>222 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>c_attn_weight = attn.c_attn.weight.detach()                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">223 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>c_attn_bias = attn.c_attn.bias.detach()                                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">224 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>new_layer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.decoder_lm_head.new_layer()                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">225 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>new_layer.add_pre_attention_layer_norm(layer.ln_1.weight.detach(),             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">parameter.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">144</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__torch_function__</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> kwargs <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">142 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>kwargs = {}                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().__torch_function__(func, types, args, kwargs)                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>144 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">'Attempted to use an uninitialized parameter in {}. '</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">'This error happens when you are using a `LazyModule` or '</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #808000; text-decoration-color: #808000\">'explicitly manipulating `torch.nn.parameter.{}` '</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Attempted to use an uninitialized parameter in <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">method</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'detach'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch._C._TensorBase'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects</span><span style=\"font-weight: bold\">&gt;</span>. \n",
       "This error happens when you are using a `LazyModule` or explicitly manipulating \n",
       "`torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to \n",
       "initialize the parameters before calling torch functions\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m11\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m# load facebook/opt-13b to NeuronCores with 2-way tensor parallel\u001b[0m                           \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m# enable float16 casting\u001b[0m                                                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m10 \u001b[0mneuron_model = GPT2ForHuggingFaceSampling.from_pretrained(\u001b[33m'\u001b[0m\u001b[33m./gpt2-split\u001b[0m\u001b[33m'\u001b[0m, batch_size=\u001b[94m1\u001b[0m,     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m11 neuron_model.to_neuron()                                                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m12 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/transformers_neuronx/gpt2/\u001b[0m\u001b[1;33mmodel.py\u001b[0m:\u001b[94m222\u001b[0m  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m in \u001b[92mto_neuron\u001b[0m                                                                                     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m219 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mlayer.materialize()                                                            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m220 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mattn = layer.attn                                                              \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m221 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mmlp = layer.mlp                                                                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m222 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0mc_attn_weight = attn.c_attn.weight.detach()                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m223 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mc_attn_bias = attn.c_attn.bias.detach()                                        \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m224 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mnew_layer = \u001b[96mself\u001b[0m.decoder_lm_head.new_layer()                                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m225 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mnew_layer.add_pre_attention_layer_norm(layer.ln_1.weight.detach(),             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/aws_neuron_venv_pytorch/lib/python3.8/site-packages/torch/nn/\u001b[0m\u001b[1;33mparameter.py\u001b[0m:\u001b[94m144\u001b[0m in            \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[92m__torch_function__\u001b[0m                                                                               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m141 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m kwargs \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mkwargs = {}                                                                \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().__torch_function__(func, types, args, kwargs)                   \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m144 \u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m145 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33m'\u001b[0m\u001b[33mAttempted to use an uninitialized parameter in \u001b[0m\u001b[33m{}\u001b[0m\u001b[33m. \u001b[0m\u001b[33m'\u001b[0m                          \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33m'\u001b[0m\u001b[33mThis error happens when you are using a `LazyModule` or \u001b[0m\u001b[33m'\u001b[0m                     \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m147 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[33m'\u001b[0m\u001b[33mexplicitly manipulating `torch.nn.parameter.\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m` \u001b[0m\u001b[33m'\u001b[0m                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mAttempted to use an uninitialized parameter in \u001b[1m<\u001b[0m\u001b[1;95mmethod\u001b[0m\u001b[39m \u001b[0m\u001b[32m'detach'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'torch._C._TensorBase'\u001b[0m\u001b[39m objects\u001b[0m\u001b[1m>\u001b[0m. \n",
       "This error happens when you are using a `LazyModule` or explicitly manipulating \n",
       "`torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to \n",
       "initialize the parameters before calling torch functions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.gpt2.model import GPT2ForHuggingFaceSampling\n",
    "\n",
    "\n",
    "\n",
    "# load facebook/opt-13b to NeuronCores with 2-way tensor parallel\n",
    "# enable float16 casting\n",
    "neuron_model = GPT2ForHuggingFaceSampling.from_pretrained('./gpt2-split', batch_size=1, tp_degree=2, amp='f32')\n",
    "neuron_model.to_neuron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fad01c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Neuron(\n",
       "  (model): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT2Neuron(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Ensures that `input_ids` and `attention_mask` are passed into the GPT2\n",
    "    model in the correct positions without requiring a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "# Create the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token # Define the padding token value\n",
    "gpt2 = GPT2Model.from_pretrained('gpt2', torchscript=True)\n",
    "model = GPT2Neuron(gpt2)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a32a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in model.parameters():\n",
    "#     print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78147a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(torch.bfloat16)\n",
    "# transformers.modeling_utils.get_parameter_dtype(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91bddb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers.modeling_utils.get_parameter_dtype(model) = lambda x: torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbdda175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper to correctly order the inputs\n",
    "model.to(torch.bfloat16)\n",
    "# Get an example input\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "\n",
    "encoded_input = tokenizer(\n",
    "    text,\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "example = (\n",
    "    encoded_input['input_ids'],\n",
    "    encoded_input['attention_mask'],\n",
    ")\n",
    "\n",
    "# Run inference on CPU\n",
    "# output_cpu = model(*example)\n",
    "\n",
    "# Compile the model using the wrapper\n",
    "model_neuron = torch_neuronx.trace(model, example)\n",
    "\n",
    "# Save the TorchScript for inference deployment\n",
    "filename = 'model.pt'\n",
    "torch.jit.save(model_neuron, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d01ffc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron outputs: tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Load the TorchScript compiled model\n",
    "model_neuron = torch.jit.load(filename)\n",
    "\n",
    "# Run inference using the Neuron model\n",
    "output_neuron = model_neuron(*example)\n",
    "\n",
    "# Compare the results\n",
    "# print(f\"CPU outputs:    {output_cpu[0][0][0][:10]}\")\n",
    "print(f\"Neuron outputs: {output_neuron[0][0][0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038256b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuron_venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
